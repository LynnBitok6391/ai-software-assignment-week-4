# Ethical Reflection

If this predictive model were deployed in a company, potential bias could occur if certain data groups are underrepresented (e.g., specific teams or departments).  
Such bias might cause unfair priority predictions, affecting which issues are solved first.  

To address this, fairness tools like **IBM AI Fairness 360** can check for statistical parity, reweigh data samples, and retrain models with bias mitigation techniques.  
Additionally, periodic audits, transparency reports, and human oversight ensure that AI systems remain fair, explainable, and inclusive.
